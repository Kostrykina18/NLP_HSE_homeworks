{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LC09Z6aPGy08"
   },
   "source": [
    "# Екатерина Кострыкина БКЛ181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3-VGnjJGy0-"
   },
   "source": [
    "#### Homework 1\n",
    "### Sentiment analysis of movie reviews using tonal dictionaries. Light version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfRdjDaFGy0-"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "YV1f_I9OGy0_",
    "outputId": "cd294026-8fe3-481e-d081-935525e926d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "session = requests.session()\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "a_morph = MorphAnalyzer()\n",
    "from natasha import Segmenter, NewsEmbedding, NewsMorphTagger, Doc, MorphVocab\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "E0LJU6xUGy1D"
   },
   "outputs": [],
   "source": [
    "def clean(review):\n",
    "    review = re.sub('[^А-Яа-яЁё ]', ' ', review)\n",
    "    review = re.sub(' +', ' ', review)\n",
    "    return review\n",
    "\n",
    "\n",
    "def load_reviews(number, ton):\n",
    "    data = []\n",
    "    for i in range(1, number+1):\n",
    "        url = f'https://www.kinopoisk.ru/reviews/type/comment/status/{ton}/period/month/page/{i}/#list'\n",
    "        req = session.get(url, headers={'User-Agent': ua.random})\n",
    "        page = req.text\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        text = soup.find_all('span', {'class': '_reachbanner_'})\n",
    "        for review in text:\n",
    "            data.append(clean(str(review))) \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_dataset(number):\n",
    "    pos_data = load_reviews(number, ton='good')\n",
    "    neg_data = load_reviews(number, ton='bad')\n",
    "    data = pd.DataFrame()\n",
    "    data['reviews'] = np.concatenate((pos_data, neg_data), axis=0)\n",
    "    a = np.zeros((len(pos_data), 1))\n",
    "    b = np.ones((len(neg_data), 1))\n",
    "    data['label'] = np.concatenate((a, b), axis=0) #0 +, 1 -\n",
    "    return data\n",
    "\n",
    "\n",
    "def lemmatize(review):\n",
    "    review = ' '.join([a_morph.parse(i)[0].normal_form for i in review])\n",
    "    return review\n",
    "\n",
    "\n",
    "def preprocess(review):\n",
    "    review = review.lower() # lowercase conversion\n",
    "    review = word_tokenize(review) # tokenize\n",
    "    review = lemmatize(review) # lemmatize\n",
    "    return review\n",
    "\n",
    "\n",
    "def get_sets(X_train, y_train):\n",
    "    pos_set = set(' '.join([i for i in X_train.loc[y_train==0.0]]).split()) # positive reviews\n",
    "    neg_set = set(' '.join([i for i in X_train.loc[y_train==1.0]]).split()) # negative reviews\n",
    "    pos_dict = pos_set.difference(neg_set) # elements which belongs only to positive reviews\n",
    "    neg_dict = neg_set.difference(pos_set) # elements which belongs only to negative reviews\n",
    "    return (pos_dict, neg_dict)\n",
    "\n",
    "\n",
    "def get_bigrams(text):\n",
    "    doc = Doc(text)\n",
    "    doc.segment(Segmenter())\n",
    "    doc.tag_morph(NewsMorphTagger(NewsEmbedding()))\n",
    "\n",
    "    lemma_tag = [[i.text, i.pos] for i in doc.tokens]\n",
    "\n",
    "    bigrams = []\n",
    "    for i in range(len(lemma_tag)-1):\n",
    "        lemma = lemma_tag[i][0]\n",
    "        next_lemma = lemma_tag[i+1][0]\n",
    "        tag = lemma_tag[i][1]\n",
    "        next_tag = lemma_tag[i+1][1]\n",
    "        if lemma == 'не' and next_tag == 'ADJ':\n",
    "            bigrams.append(' '.join([lemma, next_lemma]))\n",
    "        elif tag == 'A' and next_tag == 'VERB':\n",
    "            bigrams.append(' '.join([lemma, next_lemma]))\n",
    "        elif tag == 'ADJ' and next_tag == 'NOUN':\n",
    "            bigrams.append(' '.join([lemma, next_lemma]))\n",
    "\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def get_sets_with_bigrams(X_train, y_train):\n",
    "    pos = ' '.join([i for i in X_train.loc[y_train==0.0]]) # positive reviews\n",
    "    neg = ' '.join([i for i in X_train.loc[y_train==1.0]]) # negative reviews\n",
    "    pos_set = set(get_bigrams(pos))\n",
    "    neg_set = set(get_bigrams(neg))\n",
    "    pos_set.update(pos.split())\n",
    "    neg_set.update(neg.split())\n",
    "    pos_dict = pos_set.difference(neg_set) # elements which belongs only to positive reviews\n",
    "    neg_dict = neg_set.difference(pos_set) # elements which belongs only to negative reviews\n",
    "    return (pos_dict, neg_dict)\n",
    "\n",
    "    \n",
    "def get_tonality(review, ton_sets, prep=False):\n",
    "    if prep: # for new unprocessed inputs\n",
    "        review = preprocess(review)\n",
    "    review = set(review.split())\n",
    "    check_pos = len(review.intersection(ton_sets[0])) # size of intersection with positive set\n",
    "    check_neg = len(review.intersection(ton_sets[1])) # size of intersection with negative set\n",
    "    if check_pos > check_neg:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "def get_tonality_with_bigrams(review, ton_sets, prep=False):\n",
    "    if prep: # for new unprocessed inputs\n",
    "        review = preprocess(review)\n",
    "    bigrams = get_bigrams(review)\n",
    "    review = set(review.split())\n",
    "    review.update(bigrams)\n",
    "    check_pos = len(review.intersection(ton_sets[0])) # size of intersection with positive set\n",
    "    check_neg = len(review.intersection(ton_sets[1])) # size of intersection with negative set\n",
    "    if check_pos > check_neg:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    ans = np.mean(y_test==y_pred)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llMEmD4yGy1I"
   },
   "source": [
    "#### Creating dataset\n",
    "Since [kinopoisk.ru](https://www.kinopoisk.ru/reviews/) has 10 reviews per page, we pass the number 25 to the **get_dataset** function to download 25 pages of positive and 25 pages negative reviews (25 * 10 = 250)\n",
    "\n",
    "* 250 positive and 250 negative reviews\n",
    "\n",
    "#### Lowercase conversion, tokenize, lemmatize via **preprocess** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ND-SzFKhGy1J"
   },
   "outputs": [],
   "source": [
    "dataset = get_dataset(30)\n",
    "dataset['reviews'] = [preprocess(i) for i in dataset['reviews']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "oYof7EWSGy1N",
    "outputId": "2063fe6e-3dd9-4ac8-c244-7b3fc1a236e9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тот харди шайла лабаф и джейсон кларк играть б...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>иногда случайно среди многий тысяча тонна коро...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>есть такой человек который весь свой жизнь выс...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>картина рассказывать о учитель география котор...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>фильм рассказывать о время славянский князь гд...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>сразу оговориться что интерстеллара я понравит...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>говориться о тот что данный фильм являться про...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>я любить фильм нолан можно сказать поклонник о...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>я быть очень одушевить услышать о хороший чело...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>леон быть у я в список на просмотр давно но вс...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reviews  label\n",
       "0    тот харди шайла лабаф и джейсон кларк играть б...    0.0\n",
       "1    иногда случайно среди многий тысяча тонна коро...    0.0\n",
       "2    есть такой человек который весь свой жизнь выс...    0.0\n",
       "3    картина рассказывать о учитель география котор...    0.0\n",
       "4    фильм рассказывать о время славянский князь гд...    0.0\n",
       "..                                                 ...    ...\n",
       "595  сразу оговориться что интерстеллара я понравит...    1.0\n",
       "596  говориться о тот что данный фильм являться про...    1.0\n",
       "597  я любить фильм нолан можно сказать поклонник о...    1.0\n",
       "598  я быть очень одушевить услышать о хороший чело...    1.0\n",
       "599  леон быть у я в список на просмотр давно но вс...    1.0\n",
       "\n",
       "[600 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4ncZojLGy1S"
   },
   "source": [
    "#### Splitting into training and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "gmB5hIV_Gy1S"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset['reviews'], dataset['label'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERZuaf7cGy1X"
   },
   "source": [
    "#### Creating tonal dictionaries from a training sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "без групп биграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "S2M0HmGUGy1Z"
   },
   "outputs": [],
   "source": [
    "tonality_sets = get_sets(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "с группами биграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "B0hr3dQ2VkGC"
   },
   "outputs": [],
   "source": [
    "tonality_sets_with_bigrams = get_sets_with_bigrams(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFDGohR3Gy1d"
   },
   "source": [
    "#### Predicting tonality and counting accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "без групп биграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "fULWCSzIfTXV",
    "outputId": "749f60a2-bc89-42f9-81f2-0c1922c26ab8",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7833333333333333"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [get_tonality(i, tonality_sets) for i in X_test]\n",
    "accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "с группами биграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "B-e01pcaWjNp",
    "outputId": "fd8274d7-7d13-43dc-834d-e5a033756ca6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [get_tonality_with_bigrams(i, tonality_sets_with_bigrams) for i in X_test]\n",
    "accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpvEtGFreAjt"
   },
   "source": [
    "С использованием групп биграмм качество стало немного лучше\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PL9iLHoAhbNo"
   },
   "source": [
    "### Группы биграмм:\n",
    "1. \"не\" + прилагательное \n",
    "    - например, так как в хорошем отзыве могут встретиться \"не\" и \"плохой\", которые по отдельности могут относится к группе \"негативных\" слов, а объединив их в одно \"не плохой\" мы получим \"хороший\"\n",
    "    \n",
    "    \n",
    "2. прилагательное + существительное \n",
    "3. наречие + глагол\n",
    "    - так как прилагательные и наречия являются оценочными средствами, было бы полезно выделить к чему именно они относятся"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework_1+2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
