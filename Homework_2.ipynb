{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Екатерина Кострыкина БКЛ181\n",
    "#### Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40bS6fFVW9Ft"
   },
   "outputs": [],
   "source": [
    "#! pip install spacy\n",
    "#! python -m spacy download en_core_web_sm\n",
    "#! pip install natasha\n",
    "#! pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "I1tT39VyD_F6",
    "outputId": "d4a02681-6be5-49d2-bc89-8f841ad3817d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "2020-10-18 13:24:58,018 loading file /root/.flair/models/en-pos-ontonotes-v0.5.pt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from natasha import Segmenter, NewsEmbedding, NewsMorphTagger, Doc, MorphVocab\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "tagger = SequenceTagger.load('pos')\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRZdPNrXD_GE"
   },
   "source": [
    "В данных текстах встречаются слова - грамматические омонимы, то есть слова, которые пишутся одинакого, но имеют разные значения и **части речи**\n",
    "\n",
    "В тексте на *русском языке*:\n",
    "* стекло - существительное, глагол\n",
    "* мой - местоимение, глагол\n",
    "* три - числительное, глагол\n",
    "* косой - прилагательное, существительное\n",
    "* печь - глагол, существительное\n",
    "* мороженное - существительное, прилагательное\n",
    "* простой - существительное, глагол\n",
    "\n",
    "В тексте на *английском языке*:\n",
    "* water - поливать, вода\n",
    "* like - нравится, как\n",
    "* flat - квартира, плоский\n",
    "* sound - здоровый, звук"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "rrxE5klHD_GG"
   },
   "outputs": [],
   "source": [
    "rus_text = \"\"\"в новый дом купили новое большое стекло. мой любимый сериал закончился и мне очень грустно. вася разлил \n",
    "краску на пол и мама сказала вот теперь сиди и три. летом длинную траву убирают косой. я люблю есть мороженное каждый \n",
    "летний день. маша не любит печь пироги и торты. мама попросила достать из холодильника мороженное мясо чтобы сварить \n",
    "суп. косой заяц любит прыгать по зеленой траве и собирать вкусные грибы. дайте мне пожалуйста три банки варенья с клубникой \n",
    "и два с малиной. мне не нравится простой чай мне нравится чай с лимоном и сахаром. мама сказала сначала мой посуду потом иди гулять.\"\"\"\n",
    "\n",
    "eng_text = \"\"\"i do many sport activities so i consider myself as a sound and healthy person. i really love the sound of rain \n",
    "and of the surf. i like your new haircut it really suits you. my mom told me to water flowers every week at the morning. i \n",
    "bought a new pink car just like yours. if you want to be a healthy person you should drink at least two liters of water every \n",
    "day. your new flat is big and awesome i am happy you bought it. your humor is so flat i hate you and all your stupid jokes.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "M1yBARyuD_GK"
   },
   "outputs": [],
   "source": [
    "rus_tag = ['PREP', 'ADJ', 'NOUN', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'PRON', 'ADJ', 'NOUN', 'VERB', 'CONJ', 'PRON', 'A', 'A', \n",
    "           'NOUN', 'VERB', 'NOUN', 'PREP', 'NOUN', 'CONJ', 'NOUN', 'VERB', 'PART', 'A', 'VERB', 'CONJ', 'VERB', 'NOUN', \n",
    "           'ADJ', 'NOUN', 'VERB', 'NOUN', 'PRON', 'VERB', 'VERB', 'NOUN', 'PRON', 'ADJ', 'NOUN', 'NOUN', 'PART', 'VERB', \n",
    "           'VERB', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'VERB', 'VERB', 'PREP', 'NOUN', 'ADJ', 'NOUN', 'CONJ', 'VERB', 'NOUN', \n",
    "           'ADJ', 'NOUN', 'VERB', 'VERB', 'PREP', 'ADJ', 'NOUN', 'CONJ', 'VERB', 'ADJ', 'NOUN', 'VERB', 'PRON', 'PART', \n",
    "           'NUM', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'CONJ', 'NUM', 'PREP', 'NOUN', 'PRON', 'PART', 'VERB', 'ADJ', 'NOUN', \n",
    "           'PRON', 'VERB', 'NOUN', 'PREP', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'VERB', 'A', 'VERB', 'NOUN', 'A', 'VERB', 'VERB']\n",
    "\n",
    "eng_tag = ['PRON', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'PREP', 'PRON', 'VERB', 'PREP', 'PREP', 'DT', 'ADJ', 'CONJ', 'ADJ', 'NOUN', \n",
    "           'PRON', 'A', 'VERB', 'DT', 'NOUN', 'PREP', 'NOUN', 'CONJ', 'DT', 'PREP', 'NOUN', 'PRON', 'VERB', 'PRON', 'ADJ', \n",
    "           'NOUN', 'PRON', 'A', 'VERB', 'PRON', 'PREP', 'NOUN', 'VERB', 'PRON', 'PREP', 'VERB', 'NOUN', 'DT', 'NOUN', 'PREP', \n",
    "           'DT', 'NOUN', 'PRON', 'VERB', 'DT', 'ADJ', 'ADJ', 'NOUN', 'A', 'PREP', 'PRON', 'PREP', 'PRON', 'VERB', 'PREP', 'VERB', \n",
    "           'DT', 'ADJ', 'NOUN', 'PRON', 'VERB', 'VERB', 'A', 'A', 'NUM', 'NOUN', 'PREP', 'NOUN', 'DT', 'NOUN', 'PRON', 'ADJ', \n",
    "           'NOUN', 'VERB', 'ADJ', 'CONJ', 'ADJ', 'PRON', 'VERB', 'ADJ', 'PRON', 'VERB', 'PRON', 'PRON', 'NOUN', 'VERB', 'A', 'ADJ', \n",
    "           'PRON', 'VERB', 'PRON', 'CONJ', 'DT', 'PRON', 'ADJ', 'NOUN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словарь соответсвий тегов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "cXd9X94-D_GP"
   },
   "outputs": [],
   "source": [
    "pos_dict = {\n",
    "    'S': 'NOUN','NN': 'NOUN', 'NOUN': 'NOUN', 'NNS': 'NOUN', # NOUN\n",
    "    'V': 'VERB', 'GRND': 'VERB', 'VB': 'VERB', 'VERB': 'VERB', # VERB\n",
    "    'VBG': 'VERB', 'VBD': 'VERB', 'VBZ': 'VERB', 'INFN': 'VERB', 'VBP': 'VERB',\n",
    "    'VBN': 'VERB', 'PRTS': 'VERB', 'MD': 'VERB', 'AUX': 'VERB',\n",
    "    'ADJ': 'ADJ', 'ADJF': 'ADJ', 'A': 'ADJ', 'JJ': 'ADJ', 'JJS': 'ADJ', # ADJ\n",
    "    'ADV': 'A', 'ADVB': 'A', 'RB': 'A', 'WRB': 'A','EX': 'A', 'RBS': 'A', # A\n",
    "    'PRON': 'PRON', 'WDT': 'PRON', 'NPRO': 'PRON', 'SPRO': 'PRON', 'APRO': 'PRON', 'ADVPRO': 'PRON', # PRON\n",
    "    'PR': 'PREP', 'PREP': 'PREP', 'ADP': 'PREP', 'PRP$': 'PREP', # PREP\n",
    "    'PRP': 'PREP', 'TO': 'PREP', 'IN': 'PREP', 'DET': 'DT', 'DT': 'DT', # DT\n",
    "    'CC': 'CONJ', 'CONJ': 'CONJ', 'CCONJ': 'CONJ', 'SCONJ': 'CONJ', # CONJ\n",
    "    'NUMR': 'NUM', 'NUM': 'NUM', 'CD': 'NUM', 'ANUM': 'NUM', 'PRCL': 'PART', 'PART': 'PART' # NUM PART\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для приведения тегов к единому формату"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "x062LXz656xt"
   },
   "outputs": [],
   "source": [
    "def convert(pos_list, pos_dict):\n",
    "    conv_pos = []\n",
    "    for i in pos_list:\n",
    "        conv_pos.append(pos_dict[i])\n",
    "    return conv_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "-ftAEseSD_Gk"
   },
   "outputs": [],
   "source": [
    "# for Russian\n",
    "\n",
    "def get_Mystem_pos(text):\n",
    "    pos = []\n",
    "    text = word_tokenize(text)\n",
    "    for word in text:\n",
    "        ana = Mystem().analyze(word)\n",
    "        tag = ana[0]['analysis'][0]['gr'].split(',')[0]\n",
    "        if '=' in tag:\n",
    "            tag = tag.split('=')[0]\n",
    "            pos.append(tag)\n",
    "    return pos\n",
    "\n",
    "\n",
    "def get_pymorphy_pos(text):\n",
    "    pos = []\n",
    "    text = word_tokenize(text)\n",
    "    for word in text:\n",
    "        if word != '.':\n",
    "            pos.append(MorphAnalyzer().parse(word)[0].tag.POS)\n",
    "    return pos\n",
    "\n",
    "\n",
    "def get_natasha_pos(text):\n",
    "    doc = Doc(text)\n",
    "    doc.segment(Segmenter())\n",
    "    doc.tag_morph(NewsMorphTagger(NewsEmbedding()))\n",
    "    pos = []\n",
    "    for i in range(len(doc.tokens)):\n",
    "        if doc.tokens[i].pos != 'PUNCT':\n",
    "        pos.append(doc.tokens[i].pos)\n",
    "    return pos\n",
    "\n",
    "\n",
    "# for English\n",
    "\n",
    "def get_nltk_pos(text):\n",
    "    words = word_tokenize(eng_text)\n",
    "    t = nltk.pos_tag(words)\n",
    "    pos = []\n",
    "    for i in range(len(t)):\n",
    "        if t[i][1] != '.':\n",
    "        pos.append(t[i][1])\n",
    "    return pos\n",
    "\n",
    "\n",
    "def get_spacy_pos(text):\n",
    "    doc = nlp(text)\n",
    "    pos = []\n",
    "    for s in doc.sents:\n",
    "        for t in s:\n",
    "            if t.pos_ != 'PUNCT' and t.pos_ != 'SPACE':\n",
    "                pos.append(t.pos_)  \n",
    "    return pos\n",
    "\n",
    "\n",
    "def get_flair_pos(text):\n",
    "    sentence = Sentence(text)\n",
    "    tagger = SequenceTagger.load('pos')\n",
    "    tagger.predict(sentence)\n",
    "    pos = []\n",
    "    for i in sentence.to_tagged_string().split():\n",
    "        if '<' in i and i != '<.>' and i != '<,>':\n",
    "            pos.append(i[1:-1])\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjy83SVa31J7"
   },
   "source": [
    "Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "QwgrSqI531qL",
    "outputId": "625ef748-23f9-4a58-9183-fd219e87ade0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem_pos = get_Mystem_pos(rus_text)\n",
    "conv_mystem = convert(mystem_pos, pos_dict)\n",
    "accuracy_score(rus_tag, conv_mystem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t962dwsW32N4"
   },
   "source": [
    "Pymorpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "c5ClF0id32iJ",
    "outputId": "b7983637-fa2d-4338-8695-70bddaddd40a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pymorphy_pos = get_pymorphy_pos(rus_text)\n",
    "conv_pymorphy = convert(pymorphy_pos, pos_dict)\n",
    "accuracy_score(rus_tag, conv_pymorphy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2tcbEgmQxzq"
   },
   "source": [
    "NATASHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "ancrJyJ36XTF",
    "outputId": "91e8aeac-e650-4fa3-c466-705dc39fae7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natasha_pos = get_natasha_pos(rus_text)\n",
    "conv_natasha = convert(natasha_pos, pos_dict)\n",
    "accuracy_score(rus_tag, conv_natasha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQ-XPIpaMe-W"
   },
   "source": [
    "Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "FOipNkpGXIHM",
    "outputId": "5b84e389-78f5-45fa-8dc2-46f54a095880"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8514851485148515"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_pos = get_spacy_pos(eng_text)\n",
    "conv_spacy = convert(spacy_pos, pos_dict)\n",
    "accuracy_score(eng_tag, conv_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEpUd0P9SZ6C"
   },
   "source": [
    "Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "vw0VDobNXKXB",
    "outputId": "e3b6956f-3901-457e-c786-0ff14e1a7851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-18 12:35:44,520 loading file /root/.flair/models/en-pos-ontonotes-v0.5.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7722772277227723"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_pos = get_flair_pos(eng_text)\n",
    "conv_flair = convert(flair_pos, pos_dict)\n",
    "accuracy_score(eng_tag, conv_flair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9PSWrPFUaZ4"
   },
   "source": [
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "f8g_SCL0Ufe8",
    "outputId": "e68ab411-5ce7-440a-89ba-02ed3f1bdd46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7029702970297029"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_pos = get_nltk_pos(eng_text)\n",
    "conv_nltk = convert(nltk_pos, pos_dict)\n",
    "accuracy_score(eng_tag, conv_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQrgRZJa2gh1"
   },
   "source": [
    "### Функция поиска групп биграмм:\n",
    "1. \"не\" + прилагательное \n",
    "    - например, так как в хорошем отзыве могут встретиться \"не\" и \"плохой\", которые по отдельности могут относится к группе \"негативных\" слов, а объединив их в одно \"не плохой\" мы получим \"хороший\"\n",
    "    \n",
    "    \n",
    "2. прилагательное + существительное \n",
    "3. наречие + глагол\n",
    "    - так как прилагательные и наречия являются оценочными средствами, было бы полезно выделить к чему именно они относятся\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oif7LeFr9TLM"
   },
   "source": [
    "Так как наилучший результат был достигнут при использовании Natasha, здесь будем использовать этот парсер "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "qmiHw_4J2lCC"
   },
   "outputs": [],
   "source": [
    "def get_bigrams(text):\n",
    "    doc = Doc(text)\n",
    "    doc.segment(Segmenter())\n",
    "    doc.tag_morph(NewsMorphTagger(NewsEmbedding()))\n",
    "    for i in doc.tokens:\n",
    "        i.lemmatize(MorphVocab())\n",
    "\n",
    "    lemma_tag = [[i.lemma, i.pos] for i in doc.tokens]\n",
    "\n",
    "    bigrams = []\n",
    "    for i in range(len(lemma_tag)-1):\n",
    "        lemma = lemma_tag[i][0]\n",
    "        next_lemma = lemma_tag[i+1][0]\n",
    "        tag = lemma_tag[i][1]\n",
    "        next_tag = lemma_tag[i+1][1]\n",
    "        if lemma == 'не' and next_tag == 'ADJ':\n",
    "            bigrams.append(' '.join([lemma, next_lemma]))\n",
    "        elif tag == 'A' and next_tag == 'VERB':\n",
    "            bigrams.append(' '.join([lemma, next_lemma]))\n",
    "        elif tag == 'ADJ' and next_tag == 'NOUN':\n",
    "            bigrams.append(' '.join([lemma, next_lemma]))\n",
    "\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "hFWTMt3c8Pvj",
    "outputId": "886f6333-96c7-4e1b-8eb2-7c1bc66314a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['новый дом',\n",
       " 'большой стекло',\n",
       " 'любимый сериал',\n",
       " 'длинный трава',\n",
       " 'летний день',\n",
       " 'мороженный мясо',\n",
       " 'косой заяц',\n",
       " 'зеленый трава',\n",
       " 'вкусный гриб',\n",
       " 'простой чай']"
      ]
     },
     "execution_count": 79,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bigrams(rus_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
