{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Екатерина Кострыкина БКЛ181\n",
    "#### Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "KTCrxebfpVIH",
    "outputId": "f0cc0bf1-7f83-4899-c711-93d7c2047ebc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from string import punctuation\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_A99pg8F_Fuv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'comp.graphics' 'sci.space'\n",
      " 'talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware'\n",
      " 'comp.os.ms-windows.misc' 'rec.motorcycles' 'talk.religion.misc'\n",
      " 'misc.forsale' 'alt.atheism' 'sci.electronics' 'comp.windows.x'\n",
      " 'rec.sport.hockey' 'rec.sport.baseball' 'soc.religion.christian'\n",
      " 'talk.politics.mideast' 'talk.politics.misc' 'sci.crypt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CQw0rybM_Fn6"
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = re.sub('\\S*@\\S*\\s?', '', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(\"\\'\", \"\", text)    \n",
    "    text = ' '.join([i for i in text.split() if i not in punctuation])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3B_bXsnc_Flb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>From: (wheres my thing) Subject: WHAT car is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>From: (Guy Kuo) Subject: SI Clock Poll Final C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>From: (Thomas E Willis) Subject: PB questions....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>From: (Joe Green) Subject: Re: Weitek P9000 Or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>From: (Jonathan McDowell) Subject: Re: Shuttle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names                                               text  \n",
       "0              rec.autos  From: (wheres my thing) Subject: WHAT car is t...  \n",
       "1  comp.sys.mac.hardware  From: (Guy Kuo) Subject: SI Clock Poll Final C...  \n",
       "2  comp.sys.mac.hardware  From: (Thomas E Willis) Subject: PB questions....  \n",
       "3          comp.graphics  From: (Joe Green) Subject: Re: Weitek P9000 Or...  \n",
       "4              sci.space  From: (Jonathan McDowell) Subject: Re: Shuttle...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = [clean(i) for i in df['content']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(df.text.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [b for l in data_words for b in zip(l[:-1], l[1:])]\n",
    "freq = nltk.FreqDist(bigrams) #computes freq of occurrence\n",
    "fdist = freq.keys() # sorted according to freq\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mallet никак не работает, ни в колабе ни в джупитере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\1\\AppData\\Local\\Temp\\b53c8_corpus.txt --output C:\\Users\\1\\AppData\\Local\\Temp\\b53c8_corpus.mallet' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8eecc1fba52e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmallet_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/mallet-2.0.8/bin/mallet'\u001b[0m \u001b[1;31m# update this path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mldamallet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaMallet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmallet_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold, random_seed)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_seed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfinferencer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \"\"\"\n\u001b[1;32m--> 272\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmallet_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' train-topics --input %s --num-topics %s  --alpha %s --optimize-interval %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[1;34m'--num-threads %s --output-state %s --output-doc-topics %s --output-topic-keys %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36mconvert_input\u001b[1;34m(self, corpus, infer, serialize_corpus)\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcmd\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfcorpustxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfcorpusmallet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"converting temporary corpus to MALLET format with %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(stdout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m   1916\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m             \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1919\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\1\\AppData\\Local\\Temp\\b53c8_corpus.txt --output C:\\Users\\1\\AppData\\Local\\Temp\\b53c8_corpus.mallet' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "mallet_path = '/mallet-2.0.8/bin/mallet' # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the optimal number of topics for this dataset calculating the quality using coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_topics(number_of_topics, corpus, id2word, data_lemmatized):\n",
    "    coherence_scores = []\n",
    "    for number in number_of_topics:\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                    id2word=id2word,\n",
    "                                                    num_topics=number, \n",
    "                                                    random_state=100,\n",
    "                                                    update_every=1,\n",
    "                                                    chunksize=100,\n",
    "                                                    passes=10,\n",
    "                                                    alpha='auto',\n",
    "                                                    per_word_topics=True)\n",
    "        \n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, \n",
    "                                             texts=data_lemmatized, \n",
    "                                             dictionary=id2word, \n",
    "                                             coherence='c_v')\n",
    "        \n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        print(f'number_of_topics = {number}')\n",
    "        print(f'coherence score: {coherence_lda}')\n",
    "        print('##################')\n",
    "        coherence_scores.append((coherence_lda, number))\n",
    "        \n",
    "    coherence_scores = sorted(coherence_scores, reverse=True)\n",
    "    print(f'optimal number of topics = {coherence_scores[0][1]}')\n",
    "    print(f'best coherence score: {coherence_scores[0][0]}')\n",
    "    return coherence_scores[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_topics = 5\n",
      "coherence score: 0.45891486844074675\n",
      "##################\n",
      "number_of_topics = 8\n",
      "coherence score: 0.482693972259848\n",
      "##################\n",
      "number_of_topics = 10\n",
      "coherence score: 0.489854593921089\n",
      "##################\n",
      "number_of_topics = 15\n",
      "coherence score: 0.44662937295144844\n",
      "##################\n",
      "number_of_topics = 20\n",
      "coherence score: 0.4349993277965744\n",
      "##################\n",
      "optimal number of topics = 10\n",
      "best coherence score: 0.489854593921089\n"
     ]
    }
   ],
   "source": [
    "best_number_of_topics = get_number_of_topics([5, 8, 10, 15, 20], corpus, id2word, data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of topics for this dataset is **10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                    id2word=id2word,\n",
    "                                                    num_topics=best_number_of_topics, \n",
    "                                                    random_state=100,\n",
    "                                                    update_every=1,\n",
    "                                                    chunksize=100,\n",
    "                                                    passes=10,\n",
    "                                                    alpha='auto',\n",
    "                                                    per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model.show_topics(best_number_of_topics, num_words=50, formatted=False) \n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('year', 0.029839259),\n",
       " ('team', 0.025505481),\n",
       " ('game', 0.023295127),\n",
       " ('play', 0.017700817),\n",
       " ('win', 0.016438685),\n",
       " ('player', 0.0128361285),\n",
       " ('last', 0.010361701),\n",
       " ('choose', 0.009891362),\n",
       " ('next', 0.009800661),\n",
       " ('run', 0.009388538)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[0][1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to find the main topics for each text from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_dic(topics):\n",
    "    topic_dic = {}\n",
    "    for idx, topic in enumerate(topics):\n",
    "        dic = {}\n",
    "        for word in topic[1]:\n",
    "            dic[word[0]] = word[1]\n",
    "        topic_dic[f'{idx}'] = dic\n",
    "    return topic_dic\n",
    "\n",
    "\n",
    "def get_main_topics(data, number_of_topics):\n",
    "    main_topics = {}\n",
    "    topic_dic = get_topic_dic(topics)\n",
    "    for idx, text in enumerate(data):\n",
    "        main_topic = {}\n",
    "        for word in text:\n",
    "            for num_topic in topic_dic.keys():\n",
    "                if word in topic_dic[num_topic]:\n",
    "                    main_topic[num_topic] = float(topic_dic[num_topic][word])\n",
    "        if main_topic:\n",
    "            main_topics[idx] = sorted(main_topic.items(), key=lambda x: -x[1])[0][0]\n",
    "        else:\n",
    "            main_topics[idx] = str(random.randint(0, number_of_topics-1))\n",
    "    return main_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_topics = get_main_topics(data_lemmatized, best_number_of_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.DataFrame()\n",
    "df_text['text'] = df['text']\n",
    "df_text['lemmas'] = [' '.join(i) for i in data_lemmatized]\n",
    "df_text['main_topic'] = pd.DataFrame(main_topics, index=[0]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>main_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>From: (wheres my thing) Subject: WHAT car is t...</td>\n",
       "      <td>where thing car nntp_poste host park line wond...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>From: (Guy Kuo) Subject: SI Clock Poll Final C...</td>\n",
       "      <td>poll final call summary final call clock repor...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>From: (Thomas E Willis) Subject: PB questions....</td>\n",
       "      <td>engineering computer network distribution_usa ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>From: (Joe Green) Subject: Re: Weitek P9000 Or...</td>\n",
       "      <td>division line host write write article know ch...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>From: (Jonathan McDowell) Subject: Re: Shuttle...</td>\n",
       "      <td>question distribution article write clear caut...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11309</td>\n",
       "      <td>From: (Jim Zisfein) Subject: Re: Migraines and...</td>\n",
       "      <td>scan city reply line consultation cheap also w...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11310</td>\n",
       "      <td>From: Subject: Screen Death: Mac Plus/512 Line...</td>\n",
       "      <td>screen medford old problem screen blank someti...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11311</td>\n",
       "      <td>From: (Will Estes) Subject: Mounting CPU Coole...</td>\n",
       "      <td>este mount mail group line instal try mount co...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11312</td>\n",
       "      <td>From: (Steven Collins) Subject: Re: Sphere fro...</td>\n",
       "      <td>line article write boy embarasse trivial faq g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11313</td>\n",
       "      <td>From: (Kevin J. Gunning) Subject: stolen CBR90...</td>\n",
       "      <td>steal organization line distribution_usa host ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      From: (wheres my thing) Subject: WHAT car is t...   \n",
       "1      From: (Guy Kuo) Subject: SI Clock Poll Final C...   \n",
       "2      From: (Thomas E Willis) Subject: PB questions....   \n",
       "3      From: (Joe Green) Subject: Re: Weitek P9000 Or...   \n",
       "4      From: (Jonathan McDowell) Subject: Re: Shuttle...   \n",
       "...                                                  ...   \n",
       "11309  From: (Jim Zisfein) Subject: Re: Migraines and...   \n",
       "11310  From: Subject: Screen Death: Mac Plus/512 Line...   \n",
       "11311  From: (Will Estes) Subject: Mounting CPU Coole...   \n",
       "11312  From: (Steven Collins) Subject: Re: Sphere fro...   \n",
       "11313  From: (Kevin J. Gunning) Subject: stolen CBR90...   \n",
       "\n",
       "                                                  lemmas main_topic  \n",
       "0      where thing car nntp_poste host park line wond...          3  \n",
       "1      poll final call summary final call clock repor...          6  \n",
       "2      engineering computer network distribution_usa ...          1  \n",
       "3      division line host write write article know ch...          9  \n",
       "4      question distribution article write clear caut...          4  \n",
       "...                                                  ...        ...  \n",
       "11309  scan city reply line consultation cheap also w...          9  \n",
       "11310  screen medford old problem screen blank someti...          6  \n",
       "11311  este mount mail group line instal try mount co...          7  \n",
       "11312  line article write boy embarasse trivial faq g...          1  \n",
       "11313  steal organization line distribution_usa host ...          3  \n",
       "\n",
       "[11314 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just checking that everything is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3', '6', '1', '9', '4', '8', '0', '2', '5', '7'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.main_topic.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing TF-IDF for each topic  group and getting 5 words with the highest TF-IDF for every text in topic group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_top5(text):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    feature_index = text[0,:].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [text[0, x] for x in feature_index])\n",
    "    tfidf_scores = [(feature_names[i], s) for (i, s) in tfidf_scores]\n",
    "    top5 = ', '.join([i[0] for i in sorted(tfidf_scores, key=lambda x: -x[1])[:5]])\n",
    "    return top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top5_tfidf = {}\n",
    "for topic in df_text.main_topic.unique():\n",
    "    data = df_text['lemmas'][df_text['main_topic'] == f'{topic}']\n",
    "    idxs = df_text.index[df_text['main_topic'] == f'{topic}'].tolist()\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit(data)\n",
    "    for idx in idxs:\n",
    "        text = vectorizer.transform([data.loc[idx]])\n",
    "        top5 = get_tfidf_top5(text)\n",
    "        top5_tfidf[f'{idx}'] = top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>main_topic</th>\n",
       "      <th>top5_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>From: (wheres my thing) Subject: WHAT car is t...</td>\n",
       "      <td>where thing car nntp_poste host park line wond...</td>\n",
       "      <td>3</td>\n",
       "      <td>car, door, tellme, lerxst, funky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>From: (Guy Kuo) Subject: SI Clock Poll Final C...</td>\n",
       "      <td>poll final call summary final call clock repor...</td>\n",
       "      <td>6</td>\n",
       "      <td>poll, clock, upgrade, final, speed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>From: (Thomas E Willis) Subject: PB questions....</td>\n",
       "      <td>engineering computer network distribution_usa ...</td>\n",
       "      <td>1</td>\n",
       "      <td>powerbook, display, machine, bunch, hear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>From: (Joe Green) Subject: Re: Weitek P9000 Or...</td>\n",
       "      <td>division line host write write article know ch...</td>\n",
       "      <td>9</td>\n",
       "      <td>division, chip, weitek, quadrilateral, winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>From: (Jonathan McDowell) Subject: Re: Shuttle...</td>\n",
       "      <td>question distribution article write clear caut...</td>\n",
       "      <td>4</td>\n",
       "      <td>error, warn, bug, memory, expect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  From: (wheres my thing) Subject: WHAT car is t...   \n",
       "1  From: (Guy Kuo) Subject: SI Clock Poll Final C...   \n",
       "2  From: (Thomas E Willis) Subject: PB questions....   \n",
       "3  From: (Joe Green) Subject: Re: Weitek P9000 Or...   \n",
       "4  From: (Jonathan McDowell) Subject: Re: Shuttle...   \n",
       "\n",
       "                                              lemmas main_topic  \\\n",
       "0  where thing car nntp_poste host park line wond...          3   \n",
       "1  poll final call summary final call clock repor...          6   \n",
       "2  engineering computer network distribution_usa ...          1   \n",
       "3  division line host write write article know ch...          9   \n",
       "4  question distribution article write clear caut...          4   \n",
       "\n",
       "                                      top5_tfidf  \n",
       "0               car, door, tellme, lerxst, funky  \n",
       "1             poll, clock, upgrade, final, speed  \n",
       "2       powerbook, display, machine, bunch, hear  \n",
       "3  division, chip, weitek, quadrilateral, winter  \n",
       "4               error, warn, bug, memory, expect  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text['top5_tfidf'] = pd.DataFrame(sorted(top5_tfidf.items(), key=lambda x: (int(x[0]), x[1])))[1]\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.to_csv('homework_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Это мера, используемая для оценки тематических моделей, то есть методов, которые автоматически генерируют темы из коллекции документов.\n",
    "* Определяется как среднее или медианное значение всех косинусных близостей. \n",
    "* Чем выше coherence score, тем более связными получились темы."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Homework_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
